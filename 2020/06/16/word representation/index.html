<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
<meta name="referrer" content="never" /> â€‹â€‹â€‹â€‹
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Word representation is a process that transform the symbols to the machine understandable meanings. The goals of Word Representation are  Compute word similarity 12WR(Star) â‰ƒ WR(Sun)WR(Motel) â‰ƒ WR(H">
<meta property="og:type" content="article">
<meta property="og:title" content="Word Representation">
<meta property="og:url" content="http://yoursite.com/2020/06/16/word%20representation/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Word representation is a process that transform the symbols to the machine understandable meanings. The goals of Word Representation are  Compute word similarity 12WR(Star) â‰ƒ WR(Sun)WR(Motel) â‰ƒ WR(H">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/4.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/5.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/6.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/7.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/8.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/9.png">
<meta property="og:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/10.png">
<meta property="article:published_time" content="2020-06-16T03:03:50.000Z">
<meta property="article:modified_time" content="2020-06-28T02:51:52.415Z">
<meta property="article:author" content="thousfeet">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/06/16/word%20representation/notes-img/1.png">

<link rel="canonical" href="http://yoursite.com/2020/06/16/word%20representation/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Word Representation | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/16/word%20representation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="thousfeet">
      <meta itemprop="description" content="ç‚¹ä¸€æ¯æœˆå…‰">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Word Representation
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2020-06-16 11:03:50" itemprop="dateCreated datePublished" datetime="2020-06-16T11:03:50+08:00">2020-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2020-06-28 10:51:52" itemprop="dateModified" datetime="2020-06-28T10:51:52+08:00">2020-06-28</time>
              </span>

          
            <span id="/2020/06/16/word%20representation/" class="post-meta-item leancloud_visitors" data-flag-title="Word Representation" title="é˜…è¯»æ¬¡æ•°">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valineï¼š</span>
    
    <a title="valine" href="/2020/06/16/word%20representation/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/06/16/word%20representation/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Word representation is a process that transform the symbols to the machine understandable meanings. The goals of Word Representation are</p>
<ol type="1">
<li>Compute word similarity <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WR(Star) â‰ƒ WR(Sun)</span><br><span class="line">WR(Motel) â‰ƒ WR(Hotel)</span><br></pre></td></tr></table></figure></li>
<li>Infer word relation <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WR(China) âˆ’ WR(Beijing) â‰ƒ WR(Japan) - WR(Tokyo) </span><br><span class="line">WR(Man) â‰ƒ WR(King) âˆ’ WR(Queen) + WR(Woman)</span><br><span class="line">WR(Swimming) â‰ƒ WR(Walking) âˆ’ WR(Walk) + WR(Swim)</span><br></pre></td></tr></table></figure></li>
</ol>
<p>Now we start to discuss some ways of obtaining word representations.</p>
<a id="more"></a>
<h2 id="use-a-set-of-related-words">1. Use a set of related words</h2>
<p>Such as using synonyms and hypernyms to represent a word. e.g. WordNet, a resource containing synonym and hypernym sets.</p>
<p><img src="notes-img/1.png" /></p>
<p>However, lots of problems exist:</p>
<ul>
<li><p>Missing nuance</p>
(&quot;proficient&quot;, &quot;good&quot;) are synonyms only in some contexts</li>
<li><p>Missing new meanings of words</p>
Apple (fruit â†’ IT company)</li>
<li>Subjective</li>
<li>Data sparsity</li>
<li><p>Requires human labor to create and adapt</p></li>
</ul>
<h2 id="one-hot-representation">2. One-Hot Representation</h2>
<p>Regard words as discrete symbols.</p>
<p><img src="notes-img/2.png" /></p>
<ul>
<li>Vector dimension = # words in vocabulary</li>
<li>Token with greater ID value doesn't imply more or less important as compared with the token with less ID value.</li>
</ul>
<p>The problem is all the vectors are orthogonal. No natural notion of similarity for one-hot vectors.</p>
<p><span class="math display">\[ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘ ğ‘¡ğ‘ğ‘Ÿ, ğ‘ ğ‘¢ğ‘›)=(ğ‘£_{ğ‘ ğ‘¡ğ‘ğ‘Ÿ}, ğ‘£_{ğ‘ ğ‘¢ğ‘›} )=0\]</span></p>
<h2 id="represent-word-by-context">3. Represent Word by Context</h2>
<p>Core idea is that a word's meaning is given by the words that frequently appear close-by, which is one of the most successful ideas of modern statistical NLP.</p>
<blockquote>
<p>&quot;You shall know a word by the company it keeps.&quot; (J.R. Firth 1957: 11)</p>
</blockquote>
<p>2 ways to achieve this:</p>
<ul>
<li>Co-occurrence Counts</li>
<li>Words Embeddings</li>
</ul>
<h3 id="count-based-distributional-representation">Count-based distributional representation</h3>
<!-- ![](notes-img/3.png) -->
<h3 id="term-term-matrix">1) Term-Term matrix</h3>
<p>Measuring how often a word occurs with another.</p>
<p><span class="math display">\[C(i, j)=\sum_{\Delta x=-m}^{m} \sum_{x=-\Delta x}^{n-\Delta x-1}\left\{\begin{array}{c}
1, \text {if } \Delta x \neq 0, I(x)=i \text { and } I(x+\Delta x)=j \\
0, \text { otherwise }
\end{array}\right.\]</span></p>
<p>where ğ§ is the length of sentence ğ‘°, ğ¦ is the length of the window.</p>
<p><img src="notes-img/4.png" /></p>
<p>This can captures both syntactic and semantic information.</p>
<p>The shorter the windows, the more syntactic the representation (ğ‘šâˆˆ[1,3]) . The longer the windows, the more semantic the representation (ğ‘šâˆˆ[4,10]) .</p>
<h3 id="term-document-matrix">2) Term-Document matrix</h3>
<p>Measuring how often a word occurs in a document. Each document is a count vector in <span class="math inline">\(â„•^ğ‘‰\)</span> (a column) and each word is a count vector in <span class="math inline">\(â„•^ğ·\)</span> (a row).</p>
<p><img src="notes-img/5.png" /></p>
<p>In large corpora, we expect to see that two words are similar if their vectors are similar and two documents are similar if their vectors are similar.</p>
<p>Given 2 word vectors ğ‘£ and ğ‘¤, we can use the dot product to measure the similarity. We can normalize the similarity using vector's length which represents the word's frequency in documents.</p>
<p>Therefore, we often use cosine similarity in practice,</p>
<p><span class="math display">\[\operatorname{ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’\_ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(v,w)}=\frac{v^{*} w}{|v||w|}\]</span></p>
<h2 id="problems-of-count-based-representation">Problems of Count-Based Representation</h2>
<ul>
<li>Increase in size with vocabulary</li>
<li>Require a lot of storage</li>
<li>sparsity issues for those less frequent words, which will causes classification models less robust</li>
</ul>
<p>We prefer a dense vector to a sparse one, because dense vectors have a better generalization ability and short vectors are easier to use as features in machine learning.</p>
<p>One way to obtain dense vectors is using SVD (Singular Value Decomposition) discribed as below.</p>
<p>A ğ‘šÃ—ğ‘› matrix <span class="math inline">\(ğ‘€\)</span> can be represented as <span class="math inline">\(ğ‘€=ğ‘ˆÎ£ğ‘‰^âˆ—\)</span>, where <span class="math inline">\(ğ‘ˆ\)</span> is a ğ‘šÃ—ğ‘˜ matrix, rows corresponding to original rows, but ğ‘˜ columns represents a dimension in a new latent space, such that ğ‘˜ column vectors are orthogonal to each other. <span class="math inline">\(Î£\)</span> is a ğ‘˜Ã—ğ‘˜ diagonal matrix of singular values expressing the importance of each dimension. <span class="math inline">\(ğ‘‰^âˆ—\)</span> is a ğ‘˜Ã—ğ‘› matrix, columns corresponding to original columns, but ğ‘˜ rows corresponding to the singular values.</p>
<p>SVD can be applied to term-term matrix (co-occurrence matrix) to create dense vectors.</p>
<p>If ğ‘˜ is not small enough, we can keep the top-k singular values (like 300) to obtain a least-squares approximation to <span class="math inline">\(ğ‘€\)</span>. In this way, we can reduce word representation dimension.</p>
<p><img src="notes-img/6.png" /></p>
<p>The ğ‘‹ in the diagram is a term-term matrix, and each row of ğ‘Š is a ğ‘˜-dimensional representation of each word w.</p>
<p>However, SVD do poorly on word analogy and the matrix is high dimensional and very sparse. Besides, the computational cost is <span class="math inline">\(O(V^3)\)</span>, which is hard to accept.</p>
<h1 id="distributed-word-representation">Distributed Word Representation</h1>
<p>In non-distributed or local representation, each possible value has a unique representation slot, which requires a lot of memory to process a large database than the distributed approach.</p>
<p>Whereas with the distributed approach, you could store all that data with just a few memory units: Vehicle class (1= Large SUV, 0.1 = Compact, etcâ€¦), Brand, Price, Location, etc.</p>
<p>There are two main ways to get a distributed word representation:</p>
<ul>
<li>Neural Language Model</li>
<li>Word Embedding</li>
</ul>
<h2 id="neural-language-model">Neural Language Model</h2>
<p>AÂ neuralÂ languageÂ modelÂ isÂ aÂ languageÂ model based on neural networks, to learn distributed representations of words.</p>
<p><img src="notes-img/7.png" /></p>
<p>Steps: 1. Associate words with distributed vectors 2. Compute the joint probability of word sequences in terms of the feature vectors 3. Optimize the word feature vectors (embedding matrix C) and the parameters of the loss function (the last layer, map matrix W)</p>
<p>But the problems is that the vocabulary size can be very large, maybe hundreds of thousands of different words. This will causes</p>
<ul>
<li>Too many parameters
<ul>
<li>Lookup Table (Embedding Matrix)</li>
<li>Map matrix</li>
</ul></li>
<li>Too many computation
<ul>
<li>Non-linear operation (Tanh)</li>
<li>Softmax for each words every step: Suppose there are 100,000 words in the vocabulary, we need to compute 100,000 conditional probabilities every step.</li>
</ul></li>
</ul>
<h2 id="word-embedding">Word Embedding</h2>
<p>Word2vec uses shallow neural networks that associate words to distributed representations. It can capture many linguistic regularities, such as &quot;king&quot;-&quot;queue&quot;=&quot;man&quot;-&quot;woman&quot;.</p>
<p>Word2vec can utilize two architectures to produce distributed representations of words: - Continuous bag-of-words (CBOW)</p>
<p>In CBOW architecture, the model predicts the target word given a window of surrounding context words according to the bag-of-word assumption: The order of context words does not influence the prediction. <span class="math display">\[\max P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right)\]</span></p>
<ul>
<li><p>Continuous skip-gram</p>
<p>In skip-gram architecture, the model predicts the context words from the target word (predict one context word each step). <span class="math display">\[\max P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right)=\prod_{-m \leq j \leq m,j\neq0} P\left(w_{c+j} | w_{c}\right)\]</span></p></li>
</ul>
<p><img src="notes-img/8.png" /></p>
<p>Word2vec uses a sliding window of a fixed size moving along a sentence. In each window, the middle word is the target word, other words are the context words.</p>
<p>Given the context words, CBOW predicts the probabilities of the target word. While given a target word, skip-gram predicts the probabilities of the context words.</p>
<p>We use the softmax function to compute the prediction and use cross-entropy loss function to measure the distribution difference between the prediction and the ground truth. Then optimize the both embedding and map matrices by optimization algorithms.</p>
<p>However, softmax for all the words every step depends on a huge number of model parameters when the vocabulary size is very large, which is computationally impractical.</p>
<p>There are two main improvement methods for word2vec: - Negative sampling - Hierarchical softmax</p>
<h3 id="negative-sampling">Negative sampling</h3>
<p>The idea is, to only update a small percentage of the weights every step.</p>
<p>Take skip-gram for example, negative sampling aims to differentiate noisy words from the context words given the target word.</p>
<p><img src="notes-img/9.png" /></p>
<p>NCE is essentially an approximation of softmax (taking log), which converts the v-category problem into a bicategory problem. <span class="math inline">\(U(w)\)</span> is the distribution of word frequency. We sample K words (negtive sampling) based on <span class="math inline">\(P_n(w)\)</span> (using 3/4 as an exponential can reduce the impact of overly high frequency stop words on sampling). K often takes from 10~200.</p>
<h3 id="hierarchical-softmax">Hierarchical softmax</h3>
<p>The hierarchical softmax groups words by frequency with a Huffman tree, reduces the computation complexity of each step from ğ‘‰ to logğ‘‰.</p>
<p><img src="notes-img/10.png" /></p>
<p>The leaves on the tree represent all the words in the vocabulary. We consider the process of calculating conditional probabilities as the softmax of dot product of node embedding and word embedding from root to leaf node. (<span class="math inline">\(w_I\)</span> indicates the input word).</p>
<p>We consider the process of calculating the conditional probability as a serial multiplication of softmax of the vector product of each inner node embedding on the path from the root to the leaf corresponding to the target word. If it go left, the blue part of the formula get 1, else -1.</p>
<h2 id="beyond-word2vec">Beyond Word2vec</h2>
<h3 id="subword-information-fasttext">Subword Information: FastText</h3>
<p>Word2vec assigns a distinct vector to each word, which ignores the internal structure of words, performs bad when vocabulary is large and some words are rare.</p>
<p>Therefore FastText uses subword to get word embedding. It firstly represent each word as a bag of character n-gram (e.g. 3-gram subword of where is { &lt;wh, whe, her, ere, re&gt;, <where> }) then culculate word embedding as the sum of n-gram embeddings.</p>
<p><span class="math display">\[v_{w}=\sum_{g \in \mathcal{G}_{w}} z_{g}\]</span></p>
<p>It can shares the representations across words, but it can also lead to increased complexity due to a larger vocabulary (thus hierarchical softmax is needed).</p>
<h3 id="global-word-embedding-glove">Global Word Embedding: Glove</h3>
<p>All the machine learning models above only consider the words within a sliding window but ignore the global statistical information i.e. co-occurrence matrix <span class="math inline">\(X\)</span>.</p>
<p>Thus the purpose of Glove is producing a vector space with meaningful sub-structure while using global statistical information.</p>
<p>The original similarity between two word i and j which in context with each other is</p>
<p><span class="math display">\[\widehat{Q}_{i j}=\frac{\exp \left(u_{j}^{T} v_{i}\right)}{\sum_{w=1}^{W} \exp \left(u_{w}^{T} v_{i}\right)}\]</span></p>
<p>Due to the denominator is computationally huge, so we discard the normalization and get an approximation</p>
<p><span class="math display">\[\widehat{Q}_{i j}={\exp \left(u_{j}^{T} v_{i}\right)}\]</span></p>
<p>The original likelyhood function is</p>
<p><span class="math display">\[\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} \left(\hat{Q}_{i j}-X_{i j}\right)^{2}\]</span></p>
<p>Glove transform it into a weighted version and taking log</p>
<p><span class="math display">\[\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} f\left(X_{i j}\right)\left(\log \widehat{Q}_{i j}-\log X_{i j}\right)^{2}\]</span></p>
<p>where the weighting factor <span class="math inline">\(f\left(X_{i j}\right)\)</span> is co-occurrence matrix <span class="math inline">\(X_{ij}\)</span>. The reason for taking logarithm is that <span class="math inline">\(X_{ij}\)</span> is usually larger and difficult for optimization.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/21/%E5%B8%B8%E7%94%A8linux%E5%91%BD%E4%BB%A4/" rel="prev" title="å¸¸ç”¨linuxå‘½ä»¤">
      <i class="fa fa-chevron-left"></i> å¸¸ç”¨linuxå‘½ä»¤
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/18/semantic%20composition%20representation/" rel="next" title="Phrase&Sentence&Document Representation">
      Phrase&Sentence&Document Representation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#use-a-set-of-related-words"><span class="nav-number">1.</span> <span class="nav-text">1. Use a set of related words</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#one-hot-representation"><span class="nav-number">2.</span> <span class="nav-text">2. One-Hot Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#represent-word-by-context"><span class="nav-number">3.</span> <span class="nav-text">3. Represent Word by Context</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#count-based-distributional-representation"><span class="nav-number">3.1.</span> <span class="nav-text">Count-based distributional representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#term-term-matrix"><span class="nav-number">3.2.</span> <span class="nav-text">1) Term-Term matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#term-document-matrix"><span class="nav-number">3.3.</span> <span class="nav-text">2) Term-Document matrix</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#problems-of-count-based-representation"><span class="nav-number">4.</span> <span class="nav-text">Problems of Count-Based Representation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#distributed-word-representation"><span class="nav-number"></span> <span class="nav-text">Distributed Word Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-language-model"><span class="nav-number">1.</span> <span class="nav-text">Neural Language Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word-embedding"><span class="nav-number">2.</span> <span class="nav-text">Word Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#negative-sampling"><span class="nav-number">2.1.</span> <span class="nav-text">Negative sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-softmax"><span class="nav-number">2.2.</span> <span class="nav-text">Hierarchical softmax</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-word2vec"><span class="nav-number">3.</span> <span class="nav-text">Beyond Word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#subword-information-fasttext"><span class="nav-number">3.1.</span> <span class="nav-text">Subword Information: FastText</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#global-word-embedding-glove"><span class="nav-number">3.2.</span> <span class="nav-text">Global Word Embedding: Glove</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="thousfeet"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">thousfeet</p>
  <div class="site-description" itemprop="description">ç‚¹ä¸€æ¯æœˆå…‰</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">thousfeet</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : '3IqmK7DaHDXlbtnr4LesniUw-gzGzoHsz',
      appKey     : '2WXB4i1BEcwgiTF6CoPg5wio',
      placeholder: "Comments here",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
